{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0,2.0,3.0]\n",
    "y_data = [2.0,4.0,6.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.3\n",
    "w2 = 1.0\n",
    "b = 1.0\n",
    "epochRange = 500\n",
    "learningRate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return ((x*x*w2) + (x*w1) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,y):\n",
    "    yPred = forward(x)\n",
    "    return (yPred-y)*(yPred-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad1(x,y): #dloss/dw1\n",
    "    return (2*x*((x*w1)+(x*x*w2)-y))\n",
    "def grad2(x,y): #dloss/dw2\n",
    "    return (2*x*x*((x*w1)+(x*x*w2)-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result before training:  11.000000000012717\n",
      "step:  0  w1:  1.9999999999946785  w2:  1.5639621094251537e-12  loss:  0.9999999999962235\n",
      "step:  1  w1:  1.9999999999947717  w2:  1.5366017732062915e-12  loss:  0.9999999999962892\n",
      "step:  2  w1:  1.9999999999948637  w2:  1.5095389767580289e-12  loss:  0.9999999999963531\n",
      "step:  3  w1:  1.9999999999949538  w2:  1.4831334323403441e-12  loss:  0.9999999999964206\n",
      "step:  4  w1:  1.9999999999950424  w2:  1.4571231273194259e-12  loss:  0.9999999999964828\n",
      "step:  5  w1:  1.9999999999951295  w2:  1.431352630471827e-12  loss:  0.9999999999965414\n",
      "step:  6  w1:  1.999999999995215  w2:  1.406274912791594e-12  loss:  0.9999999999966036\n",
      "step:  7  w1:  1.9999999999952989  w2:  1.3816590478896052e-12  loss:  0.9999999999966622\n",
      "step:  8  w1:  1.9999999999953813  w2:  1.3574028952475949e-12  loss:  0.9999999999967208\n",
      "step:  9  w1:  1.9999999999954623  w2:  1.3336707678732077e-12  loss:  0.9999999999967795\n",
      "step:  10  w1:  1.999999999995542  w2:  1.3100363401249874e-12  loss:  0.9999999999968328\n",
      "step:  11  w1:  1.9999999999956204  w2:  1.2871790684940046e-12  loss:  0.9999999999968914\n",
      "step:  12  w1:  1.9999999999956974  w2:  1.2644372600575829e-12  loss:  0.9999999999969447\n",
      "step:  13  w1:  1.9999999999957732  w2:  1.242490371306793e-12  loss:  0.9999999999970033\n",
      "step:  14  w1:  1.9999999999958473  w2:  1.2203214379510778e-12  loss:  0.9999999999970512\n",
      "step:  15  w1:  1.9999999999959202  w2:  1.1992449640515938e-12  loss:  0.9999999999971081\n",
      "step:  16  w1:  1.9999999999959917  w2:  1.1778443050289193e-12  loss:  0.9999999999971507\n",
      "step:  17  w1:  1.999999999996062  w2:  1.1573939969153236e-12  loss:  0.9999999999972058\n",
      "step:  18  w1:  1.999999999996131  w2:  1.13699697950691e-12  loss:  0.999999999997252\n",
      "step:  19  w1:  1.999999999996199  w2:  1.1170085241715589e-12  loss:  0.9999999999973017\n",
      "step:  20  w1:  1.9999999999962659  w2:  1.0976062665932096e-12  loss:  0.9999999999973515\n",
      "step:  21  w1:  1.9999999999963314  w2:  1.0782350952595497e-12  loss:  0.9999999999973959\n",
      "step:  22  w1:  1.9999999999963958  w2:  1.059343540272528e-12  loss:  0.999999999997442\n",
      "step:  23  w1:  1.9999999999964588  w2:  1.0406962343509239e-12  loss:  0.9999999999974847\n",
      "step:  24  w1:  1.999999999996521  w2:  1.022421963365594e-12  loss:  0.9999999999975291\n",
      "step:  25  w1:  1.999999999996582  w2:  1.004529609100735e-12  loss:  0.9999999999975735\n",
      "step:  26  w1:  1.999999999996642  w2:  9.869481172827709e-13  loss:  0.9999999999976179\n",
      "step:  27  w1:  1.9999999999967009  w2:  9.695664656092397e-13  loss:  0.999999999997657\n",
      "step:  28  w1:  1.9999999999967588  w2:  9.525445261956866e-13  loss:  0.9999999999976978\n",
      "step:  29  w1:  1.9999999999968157  w2:  9.357712767396498e-13  loss:  0.9999999999977369\n",
      "step:  30  w1:  1.9999999999968716  w2:  9.19619752177403e-13  loss:  0.9999999999977831\n",
      "step:  31  w1:  1.9999999999969265  w2:  9.03290591931216e-13  loss:  0.9999999999978169\n",
      "step:  32  w1:  1.9999999999969804  w2:  8.875076614131446e-13  loss:  0.9999999999978577\n",
      "step:  33  w1:  1.9999999999970333  w2:  8.718846030106195e-13  loss:  0.9999999999978932\n",
      "step:  34  w1:  1.9999999999970854  w2:  8.567056338179445e-13  loss:  0.9999999999979341\n",
      "step:  35  w1:  1.9999999999971365  w2:  8.415355464094666e-13  loss:  0.9999999999979661\n",
      "step:  36  w1:  1.999999999997187  w2:  8.268184299950354e-13  loss:  0.9999999999980034\n",
      "step:  37  w1:  1.9999999999972364  w2:  8.1220345409887e-13  loss:  0.9999999999980389\n",
      "step:  38  w1:  1.9999999999972848  w2:  7.978149636997278e-13  loss:  0.9999999999980691\n",
      "step:  39  w1:  1.9999999999973326  w2:  7.839371758919133e-13  loss:  0.9999999999981064\n",
      "step:  40  w1:  1.9999999999973794  w2:  7.701792921707583e-13  loss:  0.9999999999981384\n",
      "step:  41  w1:  1.9999999999974254  w2:  7.565057853994749e-13  loss:  0.9999999999981704\n",
      "step:  42  w1:  1.9999999999974707  w2:  7.43485089766671e-13  loss:  0.9999999999982077\n",
      "step:  43  w1:  1.999999999997515  w2:  7.302823175578287e-13  loss:  0.9999999999982361\n",
      "step:  44  w1:  1.9999999999975586  w2:  7.173770851195858e-13  loss:  0.9999999999982645\n",
      "step:  45  w1:  1.9999999999976015  w2:  7.050136415173601e-13  loss:  0.9999999999982983\n",
      "step:  46  w1:  1.9999999999976437  w2:  6.924503577707018e-13  loss:  0.9999999999983267\n",
      "step:  47  w1:  1.9999999999976852  w2:  6.803578085864846e-13  loss:  0.9999999999983569\n",
      "step:  48  w1:  1.9999999999977258  w2:  6.685494764965714e-13  loss:  0.9999999999983871\n",
      "step:  49  w1:  1.9999999999977658  w2:  6.565901540753093e-13  loss:  0.9999999999984137\n",
      "step:  50  w1:  1.999999999997805  w2:  6.449949848061241e-13  loss:  0.9999999999984404\n",
      "step:  51  w1:  1.9999999999978437  w2:  6.338572274230845e-13  loss:  0.9999999999984723\n",
      "step:  52  w1:  1.9999999999978815  w2:  6.225418343561049e-13  loss:  0.9999999999984936\n",
      "step:  53  w1:  1.9999999999979188  w2:  6.117282620962559e-13  loss:  0.9999999999985238\n",
      "step:  54  w1:  1.9999999999979552  w2:  6.010257121388694e-13  loss:  0.9999999999985487\n",
      "step:  55  w1:  1.999999999997991  w2:  5.903586893182708e-13  loss:  0.9999999999985718\n",
      "step:  56  w1:  1.9999999999980262  w2:  5.800069698366659e-13  loss:  0.9999999999985967\n",
      "step:  57  w1:  1.999999999998061  w2:  5.699394674493649e-13  loss:  0.9999999999986251\n",
      "step:  58  w1:  1.999999999998095  w2:  5.59836437925276e-13  loss:  0.9999999999986482\n",
      "step:  59  w1:  1.9999999999981286  w2:  5.499865392508015e-13  loss:  0.9999999999986713\n",
      "step:  60  w1:  1.9999999999981615  w2:  5.403897714259416e-13  loss:  0.9999999999986962\n",
      "step:  61  w1:  1.9999999999981937  w2:  5.307707991405893e-13  loss:  0.9999999999987157\n",
      "step:  62  w1:  1.9999999999982254  w2:  5.215381844678065e-13  loss:  0.9999999999987406\n",
      "step:  63  w1:  1.9999999999982565  w2:  5.123677422844027e-13  loss:  0.9999999999987619\n",
      "step:  64  w1:  1.9999999999982871  w2:  5.03472635411106e-13  loss:  0.999999999998785\n",
      "step:  65  w1:  1.9999999999983171  w2:  4.945331196168243e-13  loss:  0.9999999999988045\n",
      "step:  66  w1:  1.9999999999983467  w2:  4.860243703560971e-13  loss:  0.9999999999988294\n",
      "step:  67  w1:  1.9999999999983755  w2:  4.773601898719223e-13  loss:  0.9999999999988454\n",
      "step:  68  w1:  1.9999999999984042  w2:  4.690557216477261e-13  loss:  0.9999999999988685\n",
      "step:  69  w1:  1.9999999999984321  w2:  4.608445121575986e-13  loss:  0.999999999998888\n",
      "step:  70  w1:  1.9999999999984597  w2:  4.526466253437663e-13  loss:  0.9999999999989058\n",
      "step:  71  w1:  1.9999999999984868  w2:  4.4485730060299623e-13  loss:  0.9999999999989289\n",
      "step:  72  w1:  1.9999999999985132  w2:  4.368104041205131e-13  loss:  0.9999999999989413\n",
      "step:  73  w1:  1.9999999999985394  w2:  4.292653284451606e-13  loss:  0.9999999999989626\n",
      "step:  74  w1:  1.9999999999985651  w2:  4.2170248920141407e-13  loss:  0.9999999999989821\n",
      "step:  75  w1:  1.9999999999985902  w2:  4.1432616742580453e-13  loss:  0.9999999999989981\n",
      "step:  76  w1:  1.999999999998615  w2:  4.0702089992377103e-13  loss:  0.9999999999990159\n",
      "step:  77  w1:  1.9999999999986395  w2:  3.9982665472420004e-13  loss:  0.9999999999990337\n",
      "step:  78  w1:  1.9999999999986635  w2:  3.9274787271919e-13  loss:  0.9999999999990514\n",
      "step:  79  w1:  1.999999999998687  w2:  3.8598883494527205e-13  loss:  0.9999999999990692\n",
      "step:  80  w1:  1.99999999999871  w2:  3.790743659479066e-13  loss:  0.9999999999990816\n",
      "step:  81  w1:  1.9999999999987326  w2:  3.726039861603912e-13  loss:  0.9999999999991029\n",
      "step:  82  w1:  1.9999999999987548  w2:  3.6586271195486725e-13  loss:  0.9999999999991136\n",
      "step:  83  w1:  1.9999999999987765  w2:  3.594855909014204e-13  loss:  0.9999999999991314\n",
      "step:  84  w1:  1.999999999998798  w2:  3.53406009618573e-13  loss:  0.9999999999991491\n",
      "step:  85  w1:  1.999999999998819  w2:  3.4713991086758867e-13  loss:  0.9999999999991633\n",
      "step:  86  w1:  1.9999999999988396  w2:  3.4097595263486975e-13  loss:  0.999999999999174\n",
      "step:  87  w1:  1.99999999999886  w2:  3.350695661438639e-13  loss:  0.9999999999991918\n",
      "step:  88  w1:  1.99999999999888  w2:  3.291054480555775e-13  loss:  0.9999999999992042\n",
      "step:  89  w1:  1.9999999999988995  w2:  3.234477515220877e-13  loss:  0.9999999999992202\n",
      "step:  90  w1:  1.9999999999989189  w2:  3.176168601967564e-13  loss:  0.9999999999992291\n",
      "step:  91  w1:  1.999999999998938  w2:  3.1217676737609313e-13  loss:  0.9999999999992468\n",
      "step:  92  w1:  1.9999999999989566  w2:  3.065323935188988e-13  loss:  0.9999999999992575\n",
      "step:  93  w1:  1.9999999999989748  w2:  3.0119888210859957e-13  loss:  0.9999999999992699\n",
      "step:  94  w1:  1.9999999999989928  w2:  2.9598083389286134e-13  loss:  0.9999999999992841\n",
      "step:  95  w1:  1.9999999999990106  w2:  2.910025938504421e-13  loss:  0.9999999999993019\n",
      "step:  96  w1:  1.9999999999990279  w2:  2.8558914638236993e-13  loss:  0.9999999999993072\n",
      "step:  97  w1:  1.9999999999990452  w2:  2.8077521934759524e-13  loss:  0.999999999999325\n",
      "step:  98  w1:  1.9999999999990619  w2:  2.755704938081525e-13  loss:  0.9999999999993303\n",
      "step:  99  w1:  1.9999999999990785  w2:  2.7084538461534783e-13  loss:  0.9999999999993481\n",
      "step:  100  w1:  1.9999999999990947  w2:  2.659337579544061e-13  loss:  0.9999999999993552\n",
      "step:  101  w1:  1.9999999999991107  w2:  2.6134631641665495e-13  loss:  0.9999999999993676\n",
      "step:  102  w1:  1.9999999999991263  w2:  2.566567343606383e-13  loss:  0.9999999999993783\n",
      "step:  103  w1:  1.9999999999991416  w2:  2.5230021921200917e-13  loss:  0.9999999999993907\n",
      "step:  104  w1:  1.9999999999991567  w2:  2.4788597246609957e-13  loss:  0.9999999999994031\n",
      "step:  105  w1:  1.9999999999991716  w2:  2.43489489288584e-13  loss:  0.999999999999412\n",
      "step:  106  w1:  1.9999999999991862  w2:  2.3927064179500847e-13  loss:  0.9999999999994245\n",
      "step:  107  w1:  1.9999999999992004  w2:  2.350739987619254e-13  loss:  0.9999999999994333\n",
      "step:  108  w1:  1.9999999999992144  w2:  2.308196241315618e-13  loss:  0.9999999999994404\n",
      "step:  109  w1:  1.9999999999992282  w2:  2.2670735804835022e-13  loss:  0.9999999999994493\n",
      "step:  110  w1:  1.9999999999992417  w2:  2.2290151351993516e-13  loss:  0.9999999999994635\n",
      "step:  111  w1:  1.999999999999255  w2:  2.190068511495501e-13  loss:  0.9999999999994724\n",
      "step:  112  w1:  1.999999999999268  w2:  2.1510330699496803e-13  loss:  0.9999999999994795\n",
      "step:  113  w1:  1.999999999999281  w2:  2.1135519406383348e-13  loss:  0.999999999999492\n",
      "step:  114  w1:  1.9999999999992937  w2:  2.0759819934850196e-13  loss:  0.9999999999994991\n",
      "step:  115  w1:  1.999999999999306  w2:  2.0403660388550446e-13  loss:  0.9999999999995097\n",
      "step:  116  w1:  1.999999999999318  w2:  2.0035066344374894e-13  loss:  0.999999999999515\n",
      "step:  117  w1:  1.99999999999933  w2:  1.9698002634098695e-13  loss:  0.9999999999995257\n",
      "step:  118  w1:  1.9999999999993419  w2:  1.9345395801477745e-13  loss:  0.9999999999995328\n",
      "step:  119  w1:  1.9999999999993534  w2:  1.9004779377522752e-13  loss:  0.9999999999995417\n",
      "step:  120  w1:  1.9999999999993647  w2:  1.8676153362233706e-13  loss:  0.9999999999995488\n",
      "step:  121  w1:  1.9999999999993758  w2:  1.834397463326586e-13  loss:  0.9999999999995577\n",
      "step:  122  w1:  1.999999999999387  w2:  1.8024674491383671e-13  loss:  0.9999999999995648\n",
      "step:  123  w1:  1.9999999999993978  w2:  1.7689387137946876e-13  loss:  0.9999999999995701\n",
      "step:  124  w1:  1.9999999999994083  w2:  1.7406502311272386e-13  loss:  0.9999999999995826\n",
      "step:  125  w1:  1.9999999999994187  w2:  1.7069438600996187e-13  loss:  0.9999999999995843\n",
      "step:  126  w1:  1.9999999999994291  w2:  1.6780336525383797e-13  loss:  0.999999999999595\n",
      "step:  127  w1:  1.9999999999994391  w2:  1.6488569914512303e-13  loss:  0.9999999999996039\n",
      "step:  128  w1:  1.999999999999449  w2:  1.6186145162604408e-13  loss:  0.9999999999996074\n",
      "step:  129  w1:  1.9999999999994587  w2:  1.5925020707212571e-13  loss:  0.9999999999996199\n",
      "step:  130  w1:  1.999999999999468  w2:  1.5621707776884977e-13  loss:  0.9999999999996199\n",
      "step:  131  w1:  1.9999999999994775  w2:  1.536768874885074e-13  loss:  0.9999999999996305\n",
      "step:  132  w1:  1.9999999999994866  w2:  1.508391574375655e-13  loss:  0.9999999999996341\n",
      "step:  133  w1:  1.9999999999994957  w2:  1.4817906307056362e-13  loss:  0.999999999999643\n",
      "step:  134  w1:  1.9999999999995046  w2:  1.4565663635861527e-13  loss:  0.9999999999996501\n",
      "step:  135  w1:  1.999999999999513  w2:  1.429521330706284e-13  loss:  0.9999999999996518\n",
      "step:  136  w1:  1.9999999999995217  w2:  1.4062954650311259e-13  loss:  0.9999999999996607\n",
      "step:  137  w1:  1.9999999999995302  w2:  1.380937971148687e-13  loss:  0.999999999999666\n",
      "step:  138  w1:  1.9999999999995386  w2:  1.3558025218711735e-13  loss:  0.9999999999996732\n",
      "step:  139  w1:  1.9999999999995468  w2:  1.331244388566465e-13  loss:  0.9999999999996767\n",
      "step:  140  w1:  1.9999999999995548  w2:  1.3085514299431269e-13  loss:  0.9999999999996838\n",
      "step:  141  w1:  1.9999999999995624  w2:  1.2849258839791038e-13  loss:  0.9999999999996874\n",
      "step:  142  w1:  1.9999999999995701  w2:  1.265563594429641e-13  loss:  0.999999999999698\n",
      "step:  143  w1:  1.9999999999995774  w2:  1.2409610522039477e-13  loss:  0.999999999999698\n",
      "step:  144  w1:  1.999999999999585  w2:  1.221421126970545e-13  loss:  0.9999999999997087\n",
      "step:  145  w1:  1.999999999999592  w2:  1.1989946218731168e-13  loss:  0.9999999999997105\n",
      "step:  146  w1:  1.9999999999995992  w2:  1.178078020089179e-13  loss:  0.9999999999997158\n",
      "step:  147  w1:  1.999999999999606  w2:  1.157028191542286e-13  loss:  0.9999999999997193\n",
      "step:  148  w1:  1.9999999999996132  w2:  1.137532675229868e-13  loss:  0.9999999999997282\n",
      "step:  149  w1:  1.9999999999996199  w2:  1.1168381180508551e-13  loss:  0.99999999999973\n",
      "step:  150  w1:  1.9999999999996265  w2:  1.0984084158420776e-13  loss:  0.9999999999997353\n",
      "step:  151  w1:  1.9999999999996332  w2:  1.0775806319001096e-13  loss:  0.9999999999997389\n",
      "step:  152  w1:  1.9999999999996396  w2:  1.0590621118493619e-13  loss:  0.9999999999997442\n",
      "step:  153  w1:  1.999999999999646  w2:  1.0401439115097494e-13  loss:  0.9999999999997478\n",
      "step:  154  w1:  1.9999999999996523  w2:  1.0212257111701367e-13  loss:  0.9999999999997531\n",
      "step:  155  w1:  1.9999999999996583  w2:  1.0062154958772044e-13  loss:  0.999999999999762\n",
      "step:  156  w1:  1.999999999999664  w2:  9.856985743821315e-14  loss:  0.9999999999997584\n",
      "step:  157  w1:  1.99999999999967  w2:  9.702886788003345e-14  loss:  0.9999999999997673\n",
      "step:  158  w1:  1.9999999999996758  w2:  9.52924790695197e-14  loss:  0.9999999999997708\n",
      "step:  159  w1:  1.9999999999996816  w2:  9.360049917999096e-14  loss:  0.9999999999997726\n",
      "step:  160  w1:  1.9999999999996871  w2:  9.198401445613673e-14  loss:  0.999999999999778\n",
      "step:  161  w1:  1.9999999999996927  w2:  9.029203456660798e-14  loss:  0.9999999999997815\n",
      "step:  162  w1:  1.9999999999996982  w2:  8.867999073485225e-14  loss:  0.9999999999997868\n",
      "step:  163  w1:  1.9999999999997033  w2:  8.715232385296804e-14  loss:  0.9999999999997886\n",
      "step:  164  w1:  1.9999999999997087  w2:  8.571347481305383e-14  loss:  0.9999999999997957\n",
      "step:  165  w1:  1.9999999999997138  w2:  8.399929046303258e-14  loss:  0.9999999999997939\n",
      "step:  166  w1:  1.999999999999719  w2:  8.280469048853593e-14  loss:  0.9999999999998028\n",
      "step:  167  w1:  1.9999999999997238  w2:  8.11038288148102e-14  loss:  0.9999999999998028\n",
      "step:  168  w1:  1.9999999999997289  w2:  7.976712029316151e-14  loss:  0.9999999999998082\n",
      "step:  169  w1:  1.9999999999997335  w2:  7.835491660583832e-14  loss:  0.9999999999998117\n",
      "step:  170  w1:  1.9999999999997382  w2:  7.691162667382562e-14  loss:  0.9999999999998135\n",
      "step:  171  w1:  1.9999999999997429  w2:  7.547721852600992e-14  loss:  0.999999999999817\n",
      "step:  172  w1:  1.9999999999997473  w2:  7.436699550138476e-14  loss:  0.9999999999998224\n",
      "step:  173  w1:  1.9999999999997518  w2:  7.287041486419005e-14  loss:  0.9999999999998224\n",
      "step:  174  w1:  1.9999999999997562  w2:  7.17424282711709e-14  loss:  0.9999999999998277\n",
      "step:  175  w1:  1.9999999999997604  w2:  7.03879561811282e-14  loss:  0.9999999999998295\n",
      "step:  176  w1:  1.9999999999997646  w2:  6.911786104095701e-14  loss:  0.9999999999998312\n",
      "step:  177  w1:  1.9999999999997686  w2:  6.798543355583936e-14  loss:  0.9999999999998348\n",
      "step:  178  w1:  1.9999999999997728  w2:  6.67064566314712e-14  loss:  0.9999999999998384\n",
      "step:  179  w1:  1.9999999999997766  w2:  6.563620163573253e-14  loss:  0.9999999999998419\n",
      "step:  180  w1:  1.9999999999997806  w2:  6.457926931628938e-14  loss:  0.9999999999998455\n",
      "step:  181  w1:  1.9999999999997844  w2:  6.33402604208077e-14  loss:  0.9999999999998472\n",
      "step:  182  w1:  1.9999999999997884  w2:  6.219451025939455e-14  loss:  0.999999999999849\n",
      "step:  183  w1:  1.999999999999792  w2:  6.10976099110649e-14  loss:  0.9999999999998508\n",
      "step:  184  w1:  1.9999999999997957  w2:  6.005844116001575e-14  loss:  0.9999999999998561\n",
      "step:  185  w1:  1.9999999999997993  w2:  5.90370359773606e-14  loss:  0.9999999999998579\n",
      "step:  186  w1:  1.9999999999998028  w2:  5.790904938434143e-14  loss:  0.9999999999998597\n",
      "step:  187  w1:  1.9999999999998064  w2:  5.699866650414881e-14  loss:  0.999999999999865\n",
      "step:  188  w1:  1.9999999999998097  w2:  5.598170221359216e-14  loss:  0.999999999999865\n",
      "step:  189  w1:  1.9999999999998128  w2:  5.49025654336565e-14  loss:  0.999999999999865\n",
      "step:  190  w1:  1.9999999999998161  w2:  5.3961096308774367e-14  loss:  0.9999999999998685\n",
      "step:  191  w1:  1.9999999999998195  w2:  5.291748666562672e-14  loss:  0.9999999999998685\n",
      "step:  192  w1:  1.9999999999998226  w2:  5.2291320879738135e-14  loss:  0.9999999999998774\n",
      "step:  193  w1:  1.9999999999998257  w2:  5.117221607091597e-14  loss:  0.9999999999998757\n",
      "step:  194  w1:  1.9999999999998288  w2:  5.038617816948136e-14  loss:  0.999999999999881\n",
      "step:  195  w1:  1.999999999999832  w2:  4.946247261299322e-14  loss:  0.999999999999881\n",
      "step:  196  w1:  1.9999999999998348  w2:  4.8476594567126083e-14  loss:  0.999999999999881\n",
      "step:  197  w1:  1.999999999999838  w2:  4.7632825068410974e-14  loss:  0.9999999999998845\n",
      "step:  198  w1:  1.9999999999998408  w2:  4.688231430376437e-14  loss:  0.9999999999998881\n",
      "step:  199  w1:  1.9999999999998435  w2:  4.587867268950323e-14  loss:  0.9999999999998863\n",
      "step:  200  w1:  1.9999999999998463  w2:  4.524806601151614e-14  loss:  0.9999999999998934\n",
      "step:  201  w1:  1.999999999999849  w2:  4.432436045502801e-14  loss:  0.9999999999998916\n",
      "step:  202  w1:  1.9999999999998517  w2:  4.373816269802593e-14  loss:  0.999999999999897\n",
      "step:  203  w1:  1.9999999999998541  w2:  4.2738961975863286e-14  loss:  0.9999999999998934\n",
      "step:  204  w1:  1.9999999999998568  w2:  4.21216779741717e-14  loss:  0.9999999999998987\n",
      "step:  205  w1:  1.9999999999998592  w2:  4.14066943463131e-14  loss:  0.9999999999999005\n",
      "step:  206  w1:  1.9999999999998617  w2:  4.051851592661297e-14  loss:  0.9999999999999005\n",
      "step:  207  w1:  1.999999999999864  w2:  3.9972286198497394e-14  loss:  0.9999999999999041\n",
      "step:  208  w1:  1.9999999999998666  w2:  3.920845275755528e-14  loss:  0.9999999999999041\n",
      "step:  209  w1:  1.9999999999998688  w2:  3.846238288500717e-14  loss:  0.9999999999999059\n",
      "step:  210  w1:  1.9999999999998712  w2:  3.790283048059609e-14  loss:  0.9999999999999094\n",
      "step:  211  w1:  1.9999999999998734  w2:  3.7090147226570484e-14  loss:  0.9999999999999076\n",
      "step:  212  w1:  1.9999999999998757  w2:  3.6659380693015924e-14  loss:  0.999999999999913\n",
      "step:  213  w1:  1.9999999999998777  w2:  3.597992420194533e-14  loss:  0.9999999999999147\n",
      "step:  214  w1:  1.9999999999998799  w2:  3.5327113063465736e-14  loss:  0.9999999999999147\n",
      "step:  215  w1:  1.999999999999882  w2:  3.469650638547864e-14  loss:  0.9999999999999183\n",
      "step:  216  w1:  1.999999999999884  w2:  3.4052577031196054e-14  loss:  0.9999999999999165\n",
      "step:  217  w1:  1.999999999999886  w2:  3.331094805074645e-14  loss:  0.9999999999999165\n",
      "step:  218  w1:  1.999999999999888  w2:  3.291570865397989e-14  loss:  0.9999999999999218\n",
      "step:  219  w1:  1.99999999999989  w2:  3.231174732858381e-14  loss:  0.9999999999999218\n",
      "step:  220  w1:  1.999999999999892  w2:  3.1729990463680225e-14  loss:  0.9999999999999254\n",
      "step:  221  w1:  1.999999999999894  w2:  3.129922393012567e-14  loss:  0.9999999999999272\n",
      "step:  222  w1:  1.9999999999998959  w2:  3.0535390489183564e-14  loss:  0.9999999999999236\n",
      "step:  223  w1:  1.9999999999998979  w2:  3.0033569682052995e-14  loss:  0.9999999999999272\n",
      "step:  224  w1:  1.9999999999998996  w2:  2.952286709072542e-14  loss:  0.999999999999929\n",
      "step:  225  w1:  1.9999999999999012  w2:  2.903880985198885e-14  loss:  0.999999999999929\n",
      "step:  226  w1:  1.9999999999999027  w2:  2.862136599472979e-14  loss:  0.9999999999999325\n",
      "step:  227  w1:  1.9999999999999043  w2:  2.807513626661422e-14  loss:  0.9999999999999307\n",
      "step:  228  w1:  1.999999999999906  w2:  2.759996081207465e-14  loss:  0.9999999999999343\n",
      "step:  229  w1:  1.9999999999999076  w2:  2.7275775688884103e-14  loss:  0.999999999999936\n",
      "step:  230  w1:  1.9999999999999092  w2:  2.6498619571646493e-14  loss:  0.9999999999999325\n",
      "step:  231  w1:  1.9999999999999107  w2:  2.6187757124751448e-14  loss:  0.9999999999999343\n",
      "step:  232  w1:  1.9999999999999123  w2:  2.570814077811338e-14  loss:  0.9999999999999378\n",
      "step:  233  w1:  1.999999999999914  w2:  2.5259610676164816e-14  loss:  0.9999999999999396\n",
      "step:  234  w1:  1.9999999999999156  w2:  2.487325306359526e-14  loss:  0.9999999999999414\n",
      "step:  235  w1:  1.999999999999917  w2:  2.4322582443381177e-14  loss:  0.9999999999999396\n",
      "step:  236  w1:  1.9999999999999183  w2:  2.403836534907714e-14  loss:  0.9999999999999432\n",
      "step:  237  w1:  1.9999999999999196  w2:  2.3629803276015082e-14  loss:  0.9999999999999414\n",
      "step:  238  w1:  1.9999999999999212  w2:  2.3292295476529038e-14  loss:  0.9999999999999467\n",
      "step:  239  w1:  1.9999999999999225  w2:  2.2786033777299967e-14  loss:  0.9999999999999449\n",
      "step:  240  w1:  1.9999999999999238  w2:  2.2386353488434908e-14  loss:  0.9999999999999467\n",
      "step:  241  w1:  1.9999999999999252  w2:  2.2057727473145864e-14  loss:  0.9999999999999485\n",
      "step:  242  w1:  1.9999999999999265  w2:  2.1604756479098802e-14  loss:  0.9999999999999485\n",
      "step:  243  w1:  1.9999999999999278  w2:  2.1222839758627745e-14  loss:  0.9999999999999485\n",
      "step:  244  w1:  1.9999999999999292  w2:  2.0587792188542155e-14  loss:  0.9999999999999467\n",
      "step:  245  w1:  1.9999999999999307  w2:  2.041903828879913e-14  loss:  0.999999999999952\n",
      "step:  246  w1:  1.9999999999999318  w2:  2.004600335252508e-14  loss:  0.999999999999952\n",
      "step:  247  w1:  1.999999999999933  w2:  1.962411860316752e-14  loss:  0.999999999999952\n",
      "step:  248  w1:  1.999999999999934  w2:  1.94731282718185e-14  loss:  0.9999999999999538\n",
      "step:  249  w1:  1.9999999999999352  w2:  1.8997952817278932e-14  loss:  0.9999999999999538\n",
      "step:  250  w1:  1.9999999999999363  w2:  1.8669326801989885e-14  loss:  0.9999999999999538\n",
      "step:  251  w1:  1.9999999999999376  w2:  1.8416195952375352e-14  loss:  0.9999999999999574\n",
      "step:  252  w1:  1.9999999999999387  w2:  1.7998752095116294e-14  loss:  0.9999999999999556\n",
      "step:  253  w1:  1.9999999999999398  w2:  1.7692330540319752e-14  loss:  0.9999999999999574\n",
      "step:  254  w1:  1.999999999999941  w2:  1.7297091143553196e-14  loss:  0.9999999999999556\n",
      "step:  255  w1:  1.999999999999942  w2:  1.701287404924916e-14  loss:  0.9999999999999574\n",
      "step:  256  w1:  1.9999999999999432  w2:  1.6764184091733126e-14  loss:  0.9999999999999609\n",
      "step:  257  w1:  1.9999999999999443  w2:  1.627124506879956e-14  loss:  0.9999999999999591\n",
      "step:  258  w1:  1.9999999999999454  w2:  1.620907257942055e-14  loss:  0.9999999999999645\n",
      "step:  259  w1:  1.9999999999999463  w2:  1.5858242103639002e-14  loss:  0.9999999999999627\n",
      "step:  260  w1:  1.9999999999999472  w2:  1.5622874822418468e-14  loss:  0.9999999999999645\n",
      "step:  261  w1:  1.999999999999948  w2:  1.5178785612568406e-14  loss:  0.9999999999999609\n",
      "step:  262  w1:  1.999999999999949  w2:  1.5045558849613385e-14  loss:  0.9999999999999645\n",
      "step:  263  w1:  1.9999999999999498  w2:  1.4787987107900346e-14  loss:  0.9999999999999645\n",
      "step:  264  w1:  1.9999999999999507  w2:  1.44460384163158e-14  loss:  0.9999999999999645\n",
      "step:  265  w1:  1.9999999999999516  w2:  1.4255080056080273e-14  loss:  0.9999999999999645\n",
      "step:  266  w1:  1.9999999999999523  w2:  1.410408972473125e-14  loss:  0.999999999999968\n",
      "step:  267  w1:  1.999999999999953  w2:  1.3744377464752698e-14  loss:  0.9999999999999645\n",
      "step:  268  w1:  1.9999999999999538  w2:  1.3699968543767691e-14  loss:  0.9999999999999698\n",
      "step:  269  w1:  1.9999999999999545  w2:  1.3211470412932622e-14  loss:  0.9999999999999645\n",
      "step:  270  w1:  1.9999999999999554  w2:  1.3238115765523624e-14  loss:  0.9999999999999698\n",
      "step:  271  w1:  1.999999999999956  w2:  1.2780703879378059e-14  loss:  0.999999999999968\n",
      "step:  272  w1:  1.999999999999957  w2:  1.2833994584560067e-14  loss:  0.9999999999999716\n",
      "step:  273  w1:  1.9999999999999578  w2:  1.2407668943104004e-14  loss:  0.9999999999999716\n",
      "step:  274  w1:  1.9999999999999587  w2:  1.2132333632996964e-14  loss:  0.9999999999999698\n",
      "step:  275  w1:  1.9999999999999594  w2:  1.2092365604110458e-14  loss:  0.9999999999999751\n",
      "step:  276  w1:  1.99999999999996  w2:  1.1652717286358894e-14  loss:  0.9999999999999698\n",
      "step:  277  w1:  1.9999999999999607  w2:  1.1563899444388881e-14  loss:  0.9999999999999716\n",
      "step:  278  w1:  1.9999999999999614  w2:  1.1390704652547357e-14  loss:  0.9999999999999734\n",
      "step:  279  w1:  1.9999999999999618  w2:  1.1173100939720827e-14  loss:  0.9999999999999716\n",
      "step:  280  w1:  1.9999999999999625  w2:  1.1110928450341818e-14  loss:  0.9999999999999751\n",
      "step:  281  w1:  1.9999999999999631  w2:  1.0964379011091298e-14  loss:  0.9999999999999769\n",
      "step:  282  w1:  1.9999999999999636  w2:  1.0613548535309749e-14  loss:  0.9999999999999734\n",
      "step:  283  w1:  1.9999999999999643  w2:  1.0613548535309749e-14  loss:  0.9999999999999769\n",
      "step:  284  w1:  1.999999999999965  w2:  1.0333772333104208e-14  loss:  0.9999999999999769\n",
      "step:  285  w1:  1.9999999999999656  w2:  1.0009587209913662e-14  loss:  0.9999999999999734\n",
      "step:  286  w1:  1.9999999999999662  w2:  1.0000705425716661e-14  loss:  0.9999999999999769\n",
      "step:  287  w1:  1.9999999999999667  w2:  9.907446691648148e-15  loss:  0.9999999999999787\n",
      "step:  288  w1:  1.9999999999999671  w2:  9.609906921048607e-15  loss:  0.9999999999999751\n",
      "step:  289  w1:  1.9999999999999676  w2:  9.472239265995088e-15  loss:  0.9999999999999751\n",
      "step:  290  w1:  1.999999999999968  w2:  9.48556194229059e-15  loss:  0.9999999999999787\n",
      "step:  291  w1:  1.9999999999999685  w2:  9.259076445267059e-15  loss:  0.9999999999999787\n",
      "step:  292  w1:  1.999999999999969  w2:  9.18802217169105e-15  loss:  0.9999999999999787\n",
      "step:  293  w1:  1.9999999999999694  w2:  8.881600616894508e-15  loss:  0.9999999999999751\n",
      "step:  294  w1:  1.99999999999997  w2:  8.926009537879515e-15  loss:  0.9999999999999805\n",
      "step:  295  w1:  1.9999999999999705  w2:  8.690642256658982e-15  loss:  0.9999999999999787\n",
      "step:  296  w1:  1.999999999999971  w2:  8.450834083339948e-15  loss:  0.9999999999999787\n",
      "step:  297  w1:  1.9999999999999716  w2:  8.52632924901446e-15  loss:  0.9999999999999822\n",
      "step:  298  w1:  1.999999999999972  w2:  8.162176096937408e-15  loss:  0.9999999999999805\n",
      "step:  299  w1:  1.9999999999999727  w2:  7.988981305095885e-15  loss:  0.9999999999999805\n",
      "step:  300  w1:  1.9999999999999734  w2:  7.931249707815377e-15  loss:  0.9999999999999822\n",
      "step:  301  w1:  1.9999999999999738  w2:  7.59374190832933e-15  loss:  0.9999999999999805\n",
      "step:  302  w1:  1.9999999999999745  w2:  7.45163336117731e-15  loss:  0.9999999999999822\n",
      "step:  303  w1:  1.999999999999975  w2:  7.260675000941784e-15  loss:  0.9999999999999805\n",
      "step:  304  w1:  1.9999999999999754  w2:  7.349492842911796e-15  loss:  0.999999999999984\n",
      "step:  305  w1:  1.9999999999999758  w2:  7.198502511562775e-15  loss:  0.999999999999984\n",
      "step:  306  w1:  1.9999999999999762  w2:  7.007544151327248e-15  loss:  0.999999999999984\n",
      "step:  307  w1:  1.9999999999999765  w2:  6.843231143682724e-15  loss:  0.9999999999999805\n",
      "step:  308  w1:  1.999999999999977  w2:  6.8032631147962186e-15  loss:  0.999999999999984\n",
      "step:  309  w1:  1.9999999999999771  w2:  6.754413301712712e-15  loss:  0.999999999999984\n",
      "step:  310  w1:  1.9999999999999774  w2:  6.6700363518411995e-15  loss:  0.9999999999999858\n",
      "step:  311  w1:  1.9999999999999776  w2:  6.581218509871187e-15  loss:  0.999999999999984\n",
      "step:  312  w1:  1.9999999999999778  w2:  6.487959775802674e-15  loss:  0.999999999999984\n",
      "step:  313  w1:  1.999999999999978  w2:  6.39470104173416e-15  loss:  0.999999999999984\n",
      "step:  314  w1:  1.9999999999999782  w2:  6.297001415567145e-15  loss:  0.9999999999999822\n",
      "step:  315  w1:  1.9999999999999787  w2:  6.323646768158149e-15  loss:  0.9999999999999858\n",
      "step:  316  w1:  1.999999999999979  w2:  6.110483947430118e-15  loss:  0.999999999999984\n",
      "step:  317  w1:  1.9999999999999793  w2:  6.088279486937616e-15  loss:  0.9999999999999858\n",
      "step:  318  w1:  1.9999999999999798  w2:  6.026106997558607e-15  loss:  0.9999999999999876\n",
      "step:  319  w1:  1.99999999999998  w2:  5.724126334860563e-15  loss:  0.9999999999999822\n",
      "step:  320  w1:  1.9999999999999805  w2:  5.812944176830576e-15  loss:  0.9999999999999876\n",
      "step:  321  w1:  1.999999999999981  w2:  5.626426708693549e-15  loss:  0.9999999999999876\n",
      "step:  322  w1:  1.9999999999999813  w2:  5.595340464004045e-15  loss:  0.9999999999999893\n",
      "step:  323  w1:  1.9999999999999816  w2:  5.484318161541529e-15  loss:  0.9999999999999893\n",
      "step:  324  w1:  1.9999999999999818  w2:  5.368854966980513e-15  loss:  0.9999999999999858\n",
      "step:  325  w1:  1.999999999999982  w2:  5.253391772419497e-15  loss:  0.9999999999999858\n",
      "step:  326  w1:  1.9999999999999822  w2:  5.13348768575998e-15  loss:  0.9999999999999858\n",
      "step:  327  w1:  1.9999999999999825  w2:  5.333327830192509e-15  loss:  0.9999999999999911\n",
      "step:  328  w1:  1.9999999999999827  w2:  4.9736155702139586e-15  loss:  0.9999999999999858\n",
      "step:  329  w1:  1.999999999999983  w2:  5.009142707001964e-15  loss:  0.9999999999999876\n",
      "step:  330  w1:  1.9999999999999831  w2:  5.000260922804963e-15  loss:  0.9999999999999893\n",
      "step:  331  w1:  1.9999999999999833  w2:  4.951411109721457e-15  loss:  0.9999999999999893\n",
      "step:  332  w1:  1.9999999999999836  w2:  4.867034159849944e-15  loss:  0.9999999999999876\n",
      "step:  333  w1:  1.9999999999999838  w2:  4.778216317879932e-15  loss:  0.9999999999999876\n",
      "step:  334  w1:  1.999999999999984  w2:  4.6849575838114185e-15  loss:  0.9999999999999876\n",
      "step:  335  w1:  1.9999999999999842  w2:  4.751570965288928e-15  loss:  0.9999999999999911\n",
      "step:  336  w1:  1.9999999999999845  w2:  4.582817065545904e-15  loss:  0.9999999999999893\n",
      "step:  337  w1:  1.9999999999999847  w2:  4.445149410492385e-15  loss:  0.9999999999999876\n",
      "step:  338  w1:  1.999999999999985  w2:  4.4673538709848885e-15  loss:  0.9999999999999893\n",
      "step:  339  w1:  1.9999999999999851  w2:  4.449590302590886e-15  loss:  0.9999999999999911\n",
      "step:  340  w1:  1.9999999999999853  w2:  4.231986589764356e-15  loss:  0.9999999999999876\n",
      "step:  341  w1:  1.9999999999999856  w2:  4.209782129271854e-15  loss:  0.9999999999999893\n",
      "step:  342  w1:  1.9999999999999858  w2:  4.1476096398928455e-15  loss:  0.9999999999999893\n",
      "step:  343  w1:  1.999999999999986  w2:  4.080996258415336e-15  loss:  0.9999999999999893\n",
      "step:  344  w1:  1.9999999999999862  w2:  3.978855740149821e-15  loss:  0.9999999999999893\n",
      "step:  345  w1:  1.9999999999999865  w2:  3.872274329785806e-15  loss:  0.9999999999999876\n",
      "step:  346  w1:  1.999999999999987  w2:  4.085437150513836e-15  loss:  0.9999999999999947\n",
      "step:  347  w1:  1.9999999999999871  w2:  3.6990795379442816e-15  loss:  0.9999999999999876\n",
      "step:  348  w1:  1.9999999999999876  w2:  3.703520430042782e-15  loss:  0.9999999999999929\n",
      "step:  349  w1:  1.999999999999988  w2:  3.6280252643682715e-15  loss:  0.9999999999999947\n",
      "step:  350  w1:  1.9999999999999882  w2:  3.352689954261233e-15  loss:  0.9999999999999893\n",
      "step:  351  w1:  1.9999999999999887  w2:  3.4326260120342445e-15  loss:  0.9999999999999947\n",
      "step:  352  w1:  1.999999999999989  w2:  3.272753896488222e-15  loss:  0.9999999999999911\n",
      "step:  353  w1:  1.9999999999999891  w2:  3.268313004389721e-15  loss:  0.9999999999999929\n",
      "step:  354  w1:  1.9999999999999893  w2:  3.064031967858692e-15  loss:  0.9999999999999911\n",
      "step:  355  w1:  1.9999999999999896  w2:  3.0551501836616907e-15  loss:  0.9999999999999929\n",
      "step:  356  w1:  1.9999999999999898  w2:  3.006300370578184e-15  loss:  0.9999999999999929\n",
      "step:  357  w1:  1.99999999999999  w2:  2.953009665396177e-15  loss:  0.9999999999999929\n",
      "step:  358  w1:  1.9999999999999902  w2:  2.8641918234261647e-15  loss:  0.9999999999999929\n",
      "step:  359  w1:  1.9999999999999905  w2:  2.930805204903674e-15  loss:  0.9999999999999964\n",
      "step:  360  w1:  1.9999999999999907  w2:  2.76205130516065e-15  loss:  0.9999999999999929\n",
      "step:  361  w1:  1.999999999999991  w2:  2.6288245422056314e-15  loss:  0.9999999999999929\n",
      "step:  362  w1:  1.9999999999999911  w2:  2.651029002698135e-15  loss:  0.9999999999999947\n",
      "step:  363  w1:  1.9999999999999913  w2:  2.47339331875811e-15  loss:  0.9999999999999929\n",
      "step:  364  w1:  1.9999999999999916  w2:  2.4911568871521125e-15  loss:  0.9999999999999947\n",
      "step:  365  w1:  1.9999999999999918  w2:  2.433425289871604e-15  loss:  0.9999999999999947\n",
      "step:  366  w1:  1.999999999999992  w2:  2.2113806849465727e-15  loss:  0.9999999999999911\n",
      "step:  367  w1:  1.9999999999999922  w2:  2.3446074479015915e-15  loss:  0.9999999999999964\n",
      "step:  368  w1:  1.9999999999999922  w2:  2.2735531743255818e-15  loss:  0.9999999999999947\n",
      "step:  369  w1:  1.9999999999999925  w2:  2.2069397928480722e-15  loss:  0.9999999999999929\n",
      "step:  370  w1:  1.9999999999999925  w2:  2.295757634818085e-15  loss:  0.9999999999999947\n",
      "step:  371  w1:  1.9999999999999925  w2:  2.18917622445407e-15  loss:  0.9999999999999929\n",
      "step:  372  w1:  1.9999999999999925  w2:  2.2779940664240827e-15  loss:  0.9999999999999947\n",
      "step:  373  w1:  1.9999999999999925  w2:  2.206939792848073e-15  loss:  0.9999999999999929\n",
      "step:  374  w1:  1.9999999999999925  w2:  2.2957576348180856e-15  loss:  0.9999999999999947\n",
      "step:  375  w1:  1.9999999999999925  w2:  2.189176224454071e-15  loss:  0.9999999999999929\n",
      "step:  376  w1:  1.9999999999999925  w2:  2.2779940664240835e-15  loss:  0.9999999999999947\n",
      "step:  377  w1:  1.9999999999999925  w2:  2.2069397928480738e-15  loss:  0.9999999999999929\n",
      "step:  378  w1:  1.9999999999999925  w2:  2.2957576348180864e-15  loss:  0.9999999999999947\n",
      "step:  379  w1:  1.9999999999999925  w2:  2.1891762244540717e-15  loss:  0.9999999999999929\n",
      "step:  380  w1:  1.9999999999999925  w2:  2.2779940664240843e-15  loss:  0.9999999999999947\n",
      "step:  381  w1:  1.9999999999999925  w2:  2.2069397928480746e-15  loss:  0.9999999999999929\n",
      "step:  382  w1:  1.9999999999999925  w2:  2.295757634818087e-15  loss:  0.9999999999999947\n",
      "step:  383  w1:  1.9999999999999925  w2:  2.1891762244540725e-15  loss:  0.9999999999999929\n",
      "step:  384  w1:  1.9999999999999925  w2:  2.277994066424085e-15  loss:  0.9999999999999947\n",
      "step:  385  w1:  1.9999999999999925  w2:  2.2069397928480754e-15  loss:  0.9999999999999929\n",
      "step:  386  w1:  1.9999999999999925  w2:  2.295757634818088e-15  loss:  0.9999999999999947\n",
      "step:  387  w1:  1.9999999999999925  w2:  2.1891762244540732e-15  loss:  0.9999999999999929\n",
      "step:  388  w1:  1.9999999999999925  w2:  2.277994066424086e-15  loss:  0.9999999999999947\n",
      "step:  389  w1:  1.9999999999999925  w2:  2.206939792848076e-15  loss:  0.9999999999999929\n",
      "step:  390  w1:  1.9999999999999925  w2:  2.2957576348180888e-15  loss:  0.9999999999999947\n",
      "step:  391  w1:  1.9999999999999925  w2:  2.189176224454074e-15  loss:  0.9999999999999929\n",
      "step:  392  w1:  1.9999999999999925  w2:  2.2779940664240866e-15  loss:  0.9999999999999947\n",
      "step:  393  w1:  1.9999999999999925  w2:  2.206939792848077e-15  loss:  0.9999999999999929\n",
      "step:  394  w1:  1.9999999999999925  w2:  2.2957576348180896e-15  loss:  0.9999999999999947\n",
      "step:  395  w1:  1.9999999999999925  w2:  2.189176224454075e-15  loss:  0.9999999999999929\n",
      "step:  396  w1:  1.9999999999999925  w2:  2.2779940664240874e-15  loss:  0.9999999999999947\n",
      "step:  397  w1:  1.9999999999999925  w2:  2.2069397928480777e-15  loss:  0.9999999999999929\n",
      "step:  398  w1:  1.9999999999999925  w2:  2.2957576348180903e-15  loss:  0.9999999999999947\n",
      "step:  399  w1:  1.9999999999999925  w2:  2.1891762244540756e-15  loss:  0.9999999999999929\n",
      "step:  400  w1:  1.9999999999999925  w2:  2.2779940664240882e-15  loss:  0.9999999999999947\n",
      "step:  401  w1:  1.9999999999999925  w2:  2.2069397928480785e-15  loss:  0.9999999999999929\n",
      "step:  402  w1:  1.9999999999999925  w2:  2.295757634818091e-15  loss:  0.9999999999999947\n",
      "step:  403  w1:  1.9999999999999925  w2:  2.1891762244540764e-15  loss:  0.9999999999999929\n",
      "step:  404  w1:  1.9999999999999925  w2:  2.277994066424089e-15  loss:  0.9999999999999947\n",
      "step:  405  w1:  1.9999999999999925  w2:  2.2069397928480793e-15  loss:  0.9999999999999929\n",
      "step:  406  w1:  1.9999999999999925  w2:  2.295757634818092e-15  loss:  0.9999999999999947\n",
      "step:  407  w1:  1.9999999999999925  w2:  2.189176224454077e-15  loss:  0.9999999999999929\n",
      "step:  408  w1:  1.9999999999999925  w2:  2.2779940664240898e-15  loss:  0.9999999999999947\n",
      "step:  409  w1:  1.9999999999999925  w2:  2.20693979284808e-15  loss:  0.9999999999999929\n",
      "step:  410  w1:  1.9999999999999925  w2:  2.2957576348180927e-15  loss:  0.9999999999999947\n",
      "step:  411  w1:  1.9999999999999925  w2:  2.189176224454078e-15  loss:  0.9999999999999929\n",
      "step:  412  w1:  1.9999999999999925  w2:  2.2779940664240906e-15  loss:  0.9999999999999947\n",
      "step:  413  w1:  1.9999999999999925  w2:  2.206939792848081e-15  loss:  0.9999999999999929\n",
      "step:  414  w1:  1.9999999999999925  w2:  2.2957576348180935e-15  loss:  0.9999999999999947\n",
      "step:  415  w1:  1.9999999999999925  w2:  2.1891762244540788e-15  loss:  0.9999999999999929\n",
      "step:  416  w1:  1.9999999999999925  w2:  2.2779940664240914e-15  loss:  0.9999999999999947\n",
      "step:  417  w1:  1.9999999999999925  w2:  2.2069397928480817e-15  loss:  0.9999999999999929\n",
      "step:  418  w1:  1.9999999999999925  w2:  2.2957576348180943e-15  loss:  0.9999999999999947\n",
      "step:  419  w1:  1.9999999999999925  w2:  2.1891762244540796e-15  loss:  0.9999999999999929\n",
      "step:  420  w1:  1.9999999999999925  w2:  2.277994066424092e-15  loss:  0.9999999999999947\n",
      "step:  421  w1:  1.9999999999999925  w2:  2.2069397928480825e-15  loss:  0.9999999999999929\n",
      "step:  422  w1:  1.9999999999999925  w2:  2.295757634818095e-15  loss:  0.9999999999999947\n",
      "step:  423  w1:  1.9999999999999925  w2:  2.1891762244540803e-15  loss:  0.9999999999999929\n",
      "step:  424  w1:  1.9999999999999925  w2:  2.277994066424093e-15  loss:  0.9999999999999947\n",
      "step:  425  w1:  1.9999999999999925  w2:  2.2069397928480833e-15  loss:  0.9999999999999929\n",
      "step:  426  w1:  1.9999999999999925  w2:  2.295757634818096e-15  loss:  0.9999999999999947\n",
      "step:  427  w1:  1.9999999999999925  w2:  2.189176224454081e-15  loss:  0.9999999999999929\n",
      "step:  428  w1:  1.9999999999999925  w2:  2.2779940664240937e-15  loss:  0.9999999999999947\n",
      "step:  429  w1:  1.9999999999999925  w2:  2.206939792848084e-15  loss:  0.9999999999999929\n",
      "step:  430  w1:  1.9999999999999925  w2:  2.2957576348180966e-15  loss:  0.9999999999999947\n",
      "step:  431  w1:  1.9999999999999925  w2:  2.189176224454082e-15  loss:  0.9999999999999929\n",
      "step:  432  w1:  1.9999999999999925  w2:  2.2779940664240945e-15  loss:  0.9999999999999947\n",
      "step:  433  w1:  1.9999999999999925  w2:  2.206939792848085e-15  loss:  0.9999999999999929\n",
      "step:  434  w1:  1.9999999999999925  w2:  2.2957576348180974e-15  loss:  0.9999999999999947\n",
      "step:  435  w1:  1.9999999999999925  w2:  2.1891762244540827e-15  loss:  0.9999999999999929\n",
      "step:  436  w1:  1.9999999999999925  w2:  2.2779940664240953e-15  loss:  0.9999999999999947\n",
      "step:  437  w1:  1.9999999999999925  w2:  2.2069397928480856e-15  loss:  0.9999999999999929\n",
      "step:  438  w1:  1.9999999999999925  w2:  2.2957576348180982e-15  loss:  0.9999999999999947\n",
      "step:  439  w1:  1.9999999999999925  w2:  2.1891762244540835e-15  loss:  0.9999999999999929\n",
      "step:  440  w1:  1.9999999999999925  w2:  2.277994066424096e-15  loss:  0.9999999999999947\n",
      "step:  441  w1:  1.9999999999999925  w2:  2.2069397928480864e-15  loss:  0.9999999999999929\n",
      "step:  442  w1:  1.9999999999999925  w2:  2.295757634818099e-15  loss:  0.9999999999999947\n",
      "step:  443  w1:  1.9999999999999925  w2:  2.1891762244540843e-15  loss:  0.9999999999999929\n",
      "step:  444  w1:  1.9999999999999925  w2:  2.277994066424097e-15  loss:  0.9999999999999947\n",
      "step:  445  w1:  1.9999999999999925  w2:  2.2069397928480872e-15  loss:  0.9999999999999929\n",
      "step:  446  w1:  1.9999999999999925  w2:  2.2957576348181e-15  loss:  0.9999999999999947\n",
      "step:  447  w1:  1.9999999999999925  w2:  2.189176224454085e-15  loss:  0.9999999999999929\n",
      "step:  448  w1:  1.9999999999999925  w2:  2.2779940664240977e-15  loss:  0.9999999999999947\n",
      "step:  449  w1:  1.9999999999999925  w2:  2.206939792848088e-15  loss:  0.9999999999999929\n",
      "step:  450  w1:  1.9999999999999925  w2:  2.2957576348181006e-15  loss:  0.9999999999999947\n",
      "step:  451  w1:  1.9999999999999925  w2:  2.189176224454086e-15  loss:  0.9999999999999929\n",
      "step:  452  w1:  1.9999999999999925  w2:  2.2779940664240985e-15  loss:  0.9999999999999947\n",
      "step:  453  w1:  1.9999999999999925  w2:  2.2069397928480888e-15  loss:  0.9999999999999929\n",
      "step:  454  w1:  1.9999999999999925  w2:  2.2957576348181014e-15  loss:  0.9999999999999947\n",
      "step:  455  w1:  1.9999999999999925  w2:  2.1891762244540867e-15  loss:  0.9999999999999929\n",
      "step:  456  w1:  1.9999999999999925  w2:  2.2779940664240993e-15  loss:  0.9999999999999947\n",
      "step:  457  w1:  1.9999999999999925  w2:  2.2069397928480896e-15  loss:  0.9999999999999929\n",
      "step:  458  w1:  1.9999999999999925  w2:  2.295757634818102e-15  loss:  0.9999999999999947\n",
      "step:  459  w1:  1.9999999999999925  w2:  2.1891762244540874e-15  loss:  0.9999999999999929\n",
      "step:  460  w1:  1.9999999999999925  w2:  2.2779940664241e-15  loss:  0.9999999999999947\n",
      "step:  461  w1:  1.9999999999999925  w2:  2.2069397928480904e-15  loss:  0.9999999999999929\n",
      "step:  462  w1:  1.9999999999999925  w2:  2.295757634818103e-15  loss:  0.9999999999999947\n",
      "step:  463  w1:  1.9999999999999925  w2:  2.1891762244540882e-15  loss:  0.9999999999999929\n",
      "step:  464  w1:  1.9999999999999925  w2:  2.277994066424101e-15  loss:  0.9999999999999947\n",
      "step:  465  w1:  1.9999999999999925  w2:  2.206939792848091e-15  loss:  0.9999999999999929\n",
      "step:  466  w1:  1.9999999999999925  w2:  2.2957576348181037e-15  loss:  0.9999999999999947\n",
      "step:  467  w1:  1.9999999999999925  w2:  2.189176224454089e-15  loss:  0.9999999999999929\n",
      "step:  468  w1:  1.9999999999999925  w2:  2.2779940664241016e-15  loss:  0.9999999999999947\n",
      "step:  469  w1:  1.9999999999999925  w2:  2.206939792848092e-15  loss:  0.9999999999999929\n",
      "step:  470  w1:  1.9999999999999925  w2:  2.2957576348181045e-15  loss:  0.9999999999999947\n",
      "step:  471  w1:  1.9999999999999925  w2:  2.18917622445409e-15  loss:  0.9999999999999929\n",
      "step:  472  w1:  1.9999999999999925  w2:  2.2779940664241024e-15  loss:  0.9999999999999947\n",
      "step:  473  w1:  1.9999999999999925  w2:  2.2069397928480927e-15  loss:  0.9999999999999929\n",
      "step:  474  w1:  1.9999999999999925  w2:  2.2957576348181053e-15  loss:  0.9999999999999947\n",
      "step:  475  w1:  1.9999999999999925  w2:  2.1891762244540906e-15  loss:  0.9999999999999929\n",
      "step:  476  w1:  1.9999999999999925  w2:  2.2779940664241032e-15  loss:  0.9999999999999947\n",
      "step:  477  w1:  1.9999999999999925  w2:  2.2069397928480935e-15  loss:  0.9999999999999929\n",
      "step:  478  w1:  1.9999999999999925  w2:  2.295757634818106e-15  loss:  0.9999999999999947\n",
      "step:  479  w1:  1.9999999999999925  w2:  2.1891762244540914e-15  loss:  0.9999999999999929\n",
      "step:  480  w1:  1.9999999999999925  w2:  2.277994066424104e-15  loss:  0.9999999999999947\n",
      "step:  481  w1:  1.9999999999999925  w2:  2.2069397928480943e-15  loss:  0.9999999999999929\n",
      "step:  482  w1:  1.9999999999999925  w2:  2.295757634818107e-15  loss:  0.9999999999999947\n",
      "step:  483  w1:  1.9999999999999925  w2:  2.189176224454092e-15  loss:  0.9999999999999929\n",
      "step:  484  w1:  1.9999999999999925  w2:  2.2779940664241048e-15  loss:  0.9999999999999947\n",
      "step:  485  w1:  1.9999999999999925  w2:  2.206939792848095e-15  loss:  0.9999999999999929\n",
      "step:  486  w1:  1.9999999999999925  w2:  2.2957576348181077e-15  loss:  0.9999999999999947\n",
      "step:  487  w1:  1.9999999999999925  w2:  2.189176224454093e-15  loss:  0.9999999999999929\n",
      "step:  488  w1:  1.9999999999999925  w2:  2.2779940664241056e-15  loss:  0.9999999999999947\n",
      "step:  489  w1:  1.9999999999999925  w2:  2.206939792848096e-15  loss:  0.9999999999999929\n",
      "step:  490  w1:  1.9999999999999925  w2:  2.2957576348181085e-15  loss:  0.9999999999999947\n",
      "step:  491  w1:  1.9999999999999925  w2:  2.1891762244540938e-15  loss:  0.9999999999999929\n",
      "step:  492  w1:  1.9999999999999925  w2:  2.2779940664241064e-15  loss:  0.9999999999999947\n",
      "step:  493  w1:  1.9999999999999925  w2:  2.2069397928480967e-15  loss:  0.9999999999999929\n",
      "step:  494  w1:  1.9999999999999925  w2:  2.2957576348181093e-15  loss:  0.9999999999999947\n",
      "step:  495  w1:  1.9999999999999925  w2:  2.1891762244540945e-15  loss:  0.9999999999999929\n",
      "step:  496  w1:  1.9999999999999925  w2:  2.277994066424107e-15  loss:  0.9999999999999947\n",
      "step:  497  w1:  1.9999999999999925  w2:  2.2069397928480975e-15  loss:  0.9999999999999929\n",
      "step:  498  w1:  1.9999999999999925  w2:  2.29575763481811e-15  loss:  0.9999999999999947\n",
      "step:  499  w1:  1.9999999999999925  w2:  2.1891762244540953e-15  loss:  0.9999999999999929\n",
      "result after training:  11.000000000000018\n"
     ]
    }
   ],
   "source": [
    "print(\"result before training: \", forward(5))\n",
    "for epoch in range(epochRange):\n",
    "    loss_val = 0\n",
    "    for xVal,yVal in zip(x_data,y_data):\n",
    "        gradent1 = grad1(xVal,yVal)\n",
    "        gradent2 = grad2(xVal,yVal)\n",
    "        w1 = w1 - (learningRate*gradent1)\n",
    "        w2 = w2 - (learningRate*gradent2)\n",
    "        loss_val = loss(xVal,yVal)        \n",
    "    print(\"step: \", epoch, \" w1: \", w1,\" w2: \",w2, \" loss: \", loss_val)\n",
    "print(\"result after training: \", forward(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
